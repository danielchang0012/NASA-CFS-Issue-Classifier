{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a691696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043d0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "label2int = {\n",
    "    \"bug\": 0,\n",
    "    \"documentation\" : 1,\n",
    "    \"docs\" : 1, \n",
    "    \"enhancement\" : 2,\n",
    "    \"feature\" : 2, \n",
    "    \"question\" : 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58f1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_regex = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
    "link_regex_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
    "link_regex_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
    "code_regex = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
    "\n",
    "def preprocess_raw(directory = '.', output_filepath=''):\n",
    "    \"\"\" preprocesses defect report raw data (data/raw) and saves it (data/processed)\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('preprocessing data set from raw data')\n",
    "    \n",
    "    unlabeled_df_all = None\n",
    "    labeled_df_all = None\n",
    "    \n",
    "    for file in glob.glob(os.path.join(directory, '*')):\n",
    "    \n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        unlabeled_df, labeled_df = preprocess_rows(df)\n",
    "\n",
    "        unlabeled_df.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
    "        labeled_df.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
    "\n",
    "        unlabeled_df.dropna(subset=['text'], inplace=True)\n",
    "        labeled_df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "        unlabeled_df.to_csv(os.path.join(output_filepath + '_unlabeled', Path(file).stem) + '.csv', index=False)\n",
    "            \n",
    "        unlabeled_df_all = unlabeled_df if unlabeled_df_all is None else pd.concat([unlabeled_df_all, unlabeled_df], ignore_index=True)\n",
    "        \n",
    "        labeled_df.to_csv(os.path.join(output_filepath + '_labeled', Path(file).stem + '_labeled.csv'), index=False)\n",
    "        \n",
    "        labeled_df_all = labeled_df if labeled_df_all is None else pd.concat([labeled_df_all, labeled_df], ignore_index=True)\n",
    "         \n",
    "    unlabeled_df_all.to_csv(os.path.join(output_filepath + '_unlabeled', 'all_unlabeled.csv'), index=False)\n",
    "        \n",
    "    labeled_df_all.to_csv(os.path.join(output_filepath + '_labeled', 'all_labeled.csv'), index=False)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def get_ekphrasis_preprocessor():\n",
    "    return TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    "    )\n",
    "\n",
    "def preprocess_rows(df):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('started preprocessing rows')\n",
    "\n",
    "    df = df.fillna({\n",
    "                        'Title': '',\n",
    "                        'Body': '',\n",
    "                        'Labels': ''\n",
    "                   })\n",
    "    df['text'] = df['Title'] + ' ' + df['Body']\n",
    "    \n",
    "    unlabeled_df = df[~df[\"Labels\"].str.contains(\"bug|documentation|docs|question|enhancement|feature\", regex=True)]\n",
    "    labeled_df = df[df[\"Labels\"].str.contains(\"bug|documentation|docs|question|enhancement|feature\", regex=True)]\n",
    "    \n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(labeled_df['Labels']):\n",
    "#         appear = False\n",
    "        if \"bug\" in label:\n",
    "            labels.append(\"bug\")\n",
    "#             if appear is False:\n",
    "#                 appear = True\n",
    "#             else: raise Exception(label + str(i))\n",
    "        \n",
    "        elif \"doc\" in label:\n",
    "            labels.append(\"documentation\")\n",
    "                \n",
    "        elif \"question\" in label:\n",
    "            labels.append(\"question\")\n",
    "                \n",
    "        elif \"enhancement\" in label:\n",
    "            labels.append(\"enhancement\")\n",
    "                \n",
    "        elif \"feature\" in label:\n",
    "            labels.append(\"feature\")\n",
    "        \n",
    "    labeled_df['Labels'] = labels    \n",
    "    labeled_df[\"label\"] = labeled_df['Labels'].map(label2int).tolist()\n",
    "    \n",
    "    unlabeled_df = unlabeled_df.drop([\"Labels\"], axis=1)\n",
    "    labeled_df = labeled_df.drop([\"Labels\"], axis=1)\n",
    "    \n",
    "    unlabeled_df = unlabeled_df.filter(['text', 'label'])\n",
    "    labeled_df = labeled_df.filter(['text', 'label'])\n",
    "    text_processor = get_ekphrasis_preprocessor()\n",
    "    unlabeled_df['text'] = [clean_text(text, text_processor) for text in tqdm(unlabeled_df['text'])]\n",
    "    labeled_df['text'] = [clean_text(text, text_processor) for text in tqdm(labeled_df['text'])]\n",
    "    return unlabeled_df, labeled_df\n",
    "\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException\n",
    "\n",
    "def clean_text(text, text_processor):\n",
    "    \n",
    "    #bar.set_description('regex')\n",
    "    cleaned = text\n",
    "    cleaned = cleaned.replace('**Checklist**', ' ')\n",
    "    cleaned = re.sub(r\"\\* \\[x\\] .+\\s\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\*{2}Checklist.+\\*{2}\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\*{2}.+\\*{2}\",\"<section>\", cleaned)\n",
    "    cleaned = re.sub(image_regex, r'\\1 <img>', cleaned)\n",
    "    cleaned = re.sub(link_regex_1, r'\\1 <url>', cleaned)\n",
    "    cleaned = re.sub(link_regex_2, r'\\1 <url>', cleaned)\n",
    "    cleaned = re.sub(code_regex, '<code>', cleaned)\n",
    "    \n",
    "    \n",
    "#     cleaned = cleaned.replace('**Describe the contribution**', '')\n",
    "#     cleaned = cleaned.replace('**Checklist**', '')\n",
    "#     cleaned = cleaned.replace('**Testing Performed**', '')\n",
    "#     cleaned = cleaned.replace('**Expected Behavior Changes**', '')\n",
    "#     cleaned = cleaned.replace('**Contributor Info**', '')\n",
    "    \n",
    "#     cleaned = cleaned.replace('**Describe the bug**', '')\n",
    "#     cleaned = cleaned.replace('**Expected Behavior**', '')\n",
    "#     cleaned = cleaned.replace('**Reporter Info**', '')\n",
    "#     cleaned = cleaned.replace('**Checklist (Please check before submitting)**', '')\n",
    "#     cleaned = cleaned.replace('**System(s) tested on**', '')\n",
    "#     cleaned = cleaned.replace('**Additional context**', '')\n",
    "#     cleaned = cleaned.replace('**Contributor Info**', '')\n",
    "    \n",
    "    #bar.set_description('ekph')\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    \n",
    "    signal.alarm(5)\n",
    "    \n",
    "    try:\n",
    "        cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
    "    except (RecursionError, TimeoutException):\n",
    "        cleaned = pd.NA\n",
    "    else:\n",
    "        signal.alarm(0)\n",
    "    #bar.set_description('end')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060f2e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 19:04:32,417 - __main__ - INFO - making final data set from raw data\n",
      "2023-08-15 19:04:32,419 - __main__ - INFO - preprocessing data set from raw data\n",
      "2023-08-15 19:04:32,429 - __main__ - INFO - started preprocessing rows\n",
      "/var/folders/4k/87j4hn2j03n09bmstrxm08bc0000gp/T/ipykernel_46742/1541612091.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_df['Labels'] = labels\n",
      "/var/folders/4k/87j4hn2j03n09bmstrxm08bc0000gp/T/ipykernel_46742/1541612091.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_df[\"label\"] = labeled_df['Labels'].map(label2int).tolist()\n",
      "/Users/jchang15/anaconda3/envs/defects/lib/python3.10/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jchang15/anaconda3/envs/defects/lib/python3.10/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 64/64 [00:00<00:00, 1475.30it/s]\n",
      "100%|█████████████████████████████████████████| 30/30 [00:00<00:00, 1467.86it/s]\n",
      "2023-08-15 19:04:34,596 - __main__ - INFO - started preprocessing rows\n",
      "/var/folders/4k/87j4hn2j03n09bmstrxm08bc0000gp/T/ipykernel_46742/1541612091.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_df['Labels'] = labels\n",
      "/var/folders/4k/87j4hn2j03n09bmstrxm08bc0000gp/T/ipykernel_46742/1541612091.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_df[\"label\"] = labeled_df['Labels'].map(label2int).tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 46/46 [00:00<00:00, 1440.64it/s]\n",
      "100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 2037.72it/s]\n",
      "2023-08-15 19:04:36,942 - __main__ - INFO - started preprocessing rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/87j4hn2j03n09bmstrxm08bc0000gp/T/ipykernel_46742/1541612091.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_df['Labels'] = labels\n",
      "/var/folders/4k/87j4hn2j03n09bmstrxm08bc0000gp/T/ipykernel_46742/1541612091.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_df[\"label\"] = labeled_df['Labels'].map(label2int).tolist()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m      5\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaking final data set from raw data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mpreprocess_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./issues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_filepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mpreprocess_raw\u001b[0;34m(directory, output_filepath)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     17\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file)\n\u001b[0;32m---> 19\u001b[0m     unlabeled_df, labeled_df \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     unlabeled_df\u001b[38;5;241m.\u001b[39mreplace({pd\u001b[38;5;241m.\u001b[39mNA: np\u001b[38;5;241m.\u001b[39mnan, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m     labeled_df\u001b[38;5;241m.\u001b[39mreplace({pd\u001b[38;5;241m.\u001b[39mNA: np\u001b[38;5;241m.\u001b[39mnan, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 119\u001b[0m, in \u001b[0;36mpreprocess_rows\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    117\u001b[0m unlabeled_df \u001b[38;5;241m=\u001b[39m unlabeled_df\u001b[38;5;241m.\u001b[39mfilter([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    118\u001b[0m labeled_df \u001b[38;5;241m=\u001b[39m labeled_df\u001b[38;5;241m.\u001b[39mfilter([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 119\u001b[0m text_processor \u001b[38;5;241m=\u001b[39m \u001b[43mget_ekphrasis_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m unlabeled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [clean_text(text, text_processor) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(unlabeled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m    121\u001b[0m labeled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [clean_text(text, text_processor) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(labeled_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m, in \u001b[0;36mget_ekphrasis_preprocessor\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ekphrasis_preprocessor\u001b[39m():\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTextPreProcessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# terms that will be normalized\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memail\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpercent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmoney\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mphone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumber\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# terms that will be annotated\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhashtag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallcaps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melongated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepeated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcensored\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_html\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# fix HTML tokens\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# corpus from which the word statistics are going to be used \u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# for word segmentation \u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegmenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtwitter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# corpus from which the word statistics are going to be used \u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# for spell correction\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtwitter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43munpack_hashtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# perform word segmentation on hashtags\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43munpack_contractions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Unpack contractions (can't -> can not)\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspell_correct_elong\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# spell correction for elongated words\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# select a tokenizer. You can use SocialTokenizer, or pass your own\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# the tokenizer, should take as input a string and return a list of tokens\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSocialTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlowercase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# list of dictionaries, for replacing tokens extracted from the text,\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# with other expressions. You can pass more than one dictionaries.\u001b[39;49;00m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdicts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43memoticons\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/defects/lib/python3.10/site-packages/ekphrasis/classes/preprocessor.py:95\u001b[0m, in \u001b[0;36mTextPreProcessor.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_tags \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpack_hashtags:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmenter \u001b[38;5;241m=\u001b[39m \u001b[43mSegmenter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegmenter_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfast\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspell_corrector \u001b[38;5;241m=\u001b[39m SpellCorrector(corpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrector_corpus)\n",
      "File \u001b[0;32m~/anaconda3/envs/defects/lib/python3.10/site-packages/ekphrasis/classes/segmenter.py:65\u001b[0m, in \u001b[0;36mSegmenter.__init__\u001b[0;34m(self, corpus, max_split_length)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m=\u001b[39m max_split_length\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPw \u001b[38;5;241m=\u001b[39m Pdist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munigrams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_probability)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP2w \u001b[38;5;241m=\u001b[39m \u001b[43mPdist\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbigrams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcase_split \u001b[38;5;241m=\u001b[39m ExManager()\u001b[38;5;241m.\u001b[39mget_compiled()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamel_split\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/defects/lib/python3.10/site-packages/ekphrasis/classes/segmenter.py:35\u001b[0m, in \u001b[0;36mPdist.__init__\u001b[0;34m(self, data, total, unk_func, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, count \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(total \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_prob \u001b[38;5;241m=\u001b[39m unk_func \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_unk_func\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('making final data set from raw data')\n",
    "\n",
    "\n",
    "\n",
    "preprocess_raw(directory='./issues', output_filepath='./data')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134009d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 60 with multiple labels in NASA#LC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b210ce",
   "metadata": {},
   "source": [
    "## Checking Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed93ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_unlabeled/nasa#LC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714520b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('./issues/nasa#LC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6437530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix #92, Refactor `LC_CreateTaskCDS()` to remove multiple returns\n",
      "**Checklist**\r\n",
      "* [x] I reviewed the [Contributing Guide](https://github.com/nasa/osal/blob/main/CONTRIBUTING.md).\r\n",
      "* [x] I signed and emailed the appropriate [Contributor License Agreement](https://github.com/nasa/cFS/blob/main/CONTRIBUTING.md#contributor-license-agreement-cla) to GSFC-SoftwareRelease@mail.nasa.gov and copied cfs-program@lists.nasa.gov.\r\n",
      "\r\n",
      "**Describe the contribution**\r\n",
      "- Fixes #92 \r\n",
      "  - Simple refactor removes multiple returns from `LC_CreateTaskCDS()`\r\n",
      "\r\n",
      "**Testing performed**\r\n",
      "GitHub CI actions all passing successfully (incl. Build + Run, Unit/Coverage Tests etc.).\r\n",
      "\r\n",
      "**Expected behavior changes**\r\n",
      "No change to logic/behavior\r\n",
      "\r\n",
      "**Contributor Info**\r\n",
      "Avi Weiss @thnkslprpt\n",
      " \n",
      "fix # <number> , refactor <code> to remove multiple returns <section> - fixes # <number> - simple refactor removes multiple returns from <code> <section> github ci actions all passing successfully ( incl . build + run , unit / coverage tests etc . ) . <section> no change to logic / behavior <section> avi weiss <user>\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(data1['Title'].iloc[i] + '\\n' + data1['Body'].iloc[i] + '\\n \\n' + data['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46fe15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
