{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "label2int = {\n",
    "    \"bug\": 0,\n",
    "    \"documentation\" : 1,\n",
    "    \"docs\" : 1, \n",
    "    \"enhancement\" : 2,\n",
    "    \"feature\" : 2, \n",
    "    \"question\" : 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_regex = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
    "link_regex_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
    "link_regex_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
    "code_regex = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
    "\n",
    "def preprocess_raw(directory = '.', output_filepath=''):\n",
    "    \"\"\" preprocesses defect report raw data (data/raw) and saves it (data/processed)\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('preprocessing data set from raw data')\n",
    "    \n",
    "    unlabeled_df_all = None\n",
    "    labeled_df_all = None\n",
    "    \n",
    "    for file in glob.glob(os.path.join(directory, '*')):\n",
    "    \n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        unlabeled_df, labeled_df = preprocess_rows(df)\n",
    "\n",
    "        unlabeled_df.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
    "        labeled_df.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
    "\n",
    "        unlabeled_df.dropna(subset=['text'], inplace=True)\n",
    "        labeled_df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "        unlabeled_df.to_csv(os.path.join(output_filepath + '_unlabeled', Path(file).stem) + '.csv', index=False)\n",
    "            \n",
    "        unlabeled_df_all = unlabeled_df if unlabeled_df_all is None else pd.concat([unlabeled_df_all, unlabeled_df], ignore_index=True)\n",
    "        \n",
    "        labeled_df.to_csv(os.path.join(output_filepath + '_labeled', Path(file).stem + '_labeled.csv'), index=False)\n",
    "        \n",
    "        labeled_df_all = labeled_df if labeled_df_all is None else pd.concat([labeled_df_all, labeled_df], ignore_index=True)\n",
    "         \n",
    "    unlabeled_df_all.to_csv(os.path.join(output_filepath + '_unlabeled', 'all_unlabeled.csv'), index=False)\n",
    "        \n",
    "    labeled_df_all.to_csv(os.path.join(output_filepath + '_labeled', 'all_labeled.csv'), index=False)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def get_ekphrasis_preprocessor():\n",
    "    return TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    "    )\n",
    "\n",
    "def preprocess_rows(df):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('started preprocessing rows')\n",
    "\n",
    "    df = df.fillna({\n",
    "                        'Title': '',\n",
    "                        'Body': '',\n",
    "                        'Labels': ''\n",
    "                   })\n",
    "    df['text'] = df['Title'] + ' ' + df['Body']\n",
    "    \n",
    "    unlabeled_df = df[~df[\"Labels\"].str.contains(\"bug|documentation|docs|question|enhancement|feature\", regex=True)]\n",
    "    labeled_df = df[df[\"Labels\"].str.contains(\"bug|documentation|docs|question|enhancement|feature\", regex=True)]\n",
    "    \n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(labeled_df['Labels']):\n",
    "#         appear = False\n",
    "        if \"bug\" in label:\n",
    "            labels.append(\"bug\")\n",
    "#             if appear is False:\n",
    "#                 appear = True\n",
    "#             else: raise Exception(label + str(i))\n",
    "        \n",
    "        elif \"doc\" in label:\n",
    "            labels.append(\"documentation\")\n",
    "                \n",
    "        elif \"question\" in label:\n",
    "            labels.append(\"question\")\n",
    "                \n",
    "        elif \"enhancement\" in label:\n",
    "            labels.append(\"enhancement\")\n",
    "                \n",
    "        elif \"feature\" in label:\n",
    "            labels.append(\"feature\")\n",
    "        \n",
    "    labeled_df['Labels'] = labels    \n",
    "    labeled_df[\"label\"] = labeled_df['Labels'].map(label2int).tolist()\n",
    "    \n",
    "    unlabeled_df = unlabeled_df.drop([\"Labels\"], axis=1)\n",
    "    labeled_df = labeled_df.drop([\"Labels\"], axis=1)\n",
    "    \n",
    "    unlabeled_df = unlabeled_df.filter(['text', 'label'])\n",
    "    labeled_df = labeled_df.filter(['text', 'label'])\n",
    "    text_processor = get_ekphrasis_preprocessor()\n",
    "    unlabeled_df['text'] = [clean_text(text, text_processor) for text in tqdm(unlabeled_df['text'])]\n",
    "    labeled_df['text'] = [clean_text(text, text_processor) for text in tqdm(labeled_df['text'])]\n",
    "    return unlabeled_df, labeled_df\n",
    "\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException\n",
    "\n",
    "def clean_text(text, text_processor):\n",
    "    \n",
    "    #bar.set_description('regex')\n",
    "    cleaned = text\n",
    "    cleaned = re.sub('\\*{2}Checklist.+?\\*{2}.+?\\*{2}.+?\\*{2}', ' ', cleaned, flags=re.DOTALL) # Remove Checklist\n",
    "    cleaned = re.sub(r\"\\*{2}.+\\*{2}\",\"<section>\", cleaned)\n",
    "    cleaned = re.sub(image_regex, r'\\1 <img>', cleaned)\n",
    "    cleaned = re.sub(link_regex_1, r'\\1 <url>', cleaned)\n",
    "    cleaned = re.sub(link_regex_2, r'\\1 <url>', cleaned)\n",
    "    cleaned = re.sub(code_regex, '<code>', cleaned)\n",
    "    \n",
    "    \n",
    "#     cleaned = cleaned.replace('**Describe the contribution**', '')\n",
    "#     cleaned = cleaned.replace('**Checklist**', '')\n",
    "#     cleaned = cleaned.replace('**Testing Performed**', '')\n",
    "#     cleaned = cleaned.replace('**Expected Behavior Changes**', '')\n",
    "#     cleaned = cleaned.replace('**Contributor Info**', '')\n",
    "    \n",
    "#     cleaned = cleaned.replace('**Describe the bug**', '')\n",
    "#     cleaned = cleaned.replace('**Expected Behavior**', '')\n",
    "#     cleaned = cleaned.replace('**Reporter Info**', '')\n",
    "#     cleaned = cleaned.replace('**Checklist (Please check before submitting)**', '')\n",
    "#     cleaned = cleaned.replace('**System(s) tested on**', '')\n",
    "#     cleaned = cleaned.replace('**Additional context**', '')\n",
    "#     cleaned = cleaned.replace('**Contributor Info**', '')\n",
    "    \n",
    "    #bar.set_description('ekph')\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    \n",
    "    signal.alarm(5)\n",
    "    \n",
    "    try:\n",
    "        cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
    "    except (RecursionError, TimeoutException):\n",
    "        cleaned = pd.NA\n",
    "    else:\n",
    "        signal.alarm(0)\n",
    "    #bar.set_description('end')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f2e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('making final data set from raw data')\n",
    "\n",
    "\n",
    "\n",
    "preprocess_raw(directory='./issues', output_filepath='./data')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134009d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 60 with multiple labels in NASA#osal, with ~700 issues labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bef9d9",
   "metadata": {},
   "source": [
    "## Checking Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed93ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_unlabeled/nasa#osal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714520b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('./issues/nasa#osal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6437530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix #1402, Fixes errors in IC Bundle workflow file\n",
      "**Checklist (Please check before submitting)**\n",
      "\n",
      "* [X] I reviewed the [Contributing Guide](https://github.com/nasa/osal/blob/main/CONTRIBUTING.md).\n",
      "* [X] I signed and emailed the appropriate [Contributor License Agreement](https://github.com/nasa/cFS/blob/main/CONTRIBUTING.md#contributor-license-agreement-cla) to GSFC-SoftwareRelease@mail.nasa.gov and copied cfs-program@lists.nasa.gov.\n",
      "\n",
      "**Describe the contribution**\n",
      "Addresses issue #1402.\n",
      "\n",
      "**Testing performed**\n",
      "Generated IC branch in fork.\n",
      "\n",
      "**Expected behavior changes**\n",
      "No additional \"#\" is placed in front of \"#Changelog\" in Changelog.md\n",
      "No additional \"#\" is placed in front of \"#define OS_BUILD_NUMBER ...\" in src/os/inc/osapi-version.h\n",
      "Additional occurrences of the string \"Changelog\" are not replaced in the Changelog.md file\n",
      "\n",
      "**System(s) tested on**\n",
      "GitHub\n",
      "\n",
      "**Contributor Info - All information REQUIRED for consideration of pull request**\n",
      "Dylan Z. Baker/NASA GSFC\n",
      " \n",
      "fix # <number> , fixes errors in ic bundle workflow file addresses issue # <number> . <section> generated ic branch in fork . <section> no additional \" # \" is placed in front of \" <hashtag> changelog </hashtag> \" in changelog . md no additional \" # \" is placed in front of \" <hashtag> define </hashtag> os_build_number . <repeated> \" in src / os / inc / osapi - version . h additional occurrences of the string \" changelog \" are not replaced in the changelog . md file <section> github <section> dylan z . baker / <allcaps> nasa gsfc </allcaps>\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(data1['Title'].iloc[i] + '\\n' + data1['Body'].iloc[i] + '\\n \\n' + data['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aabf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
