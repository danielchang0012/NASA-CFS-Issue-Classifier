{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a691696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043d0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mapping\n",
    "label2int = {\n",
    "    \"bug\": 0,\n",
    "    \"documentation\" : 1,\n",
    "    \"docs\" : 1, \n",
    "    \"enhancement\" : 2,\n",
    "    \"feature\" : 2, \n",
    "    \"question\" : 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58f1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_regex = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
    "link_regex_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
    "link_regex_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
    "code_regex = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
    "\n",
    "def preprocess_raw(directory = '.', output_filepath=''):\n",
    "    \"\"\" preprocesses defect report raw data (data/raw) and saves it (data/processed)\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('preprocessing data set from raw data')\n",
    "    \n",
    "    unlabeled_df_all = None\n",
    "    labeled_df_all = None\n",
    "    \n",
    "    for file in glob.glob(os.path.join(directory, '*')):\n",
    "    \n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        unlabeled_df, labeled_df = preprocess_rows(df)\n",
    "\n",
    "        unlabeled_df.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
    "        labeled_df.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
    "\n",
    "        unlabeled_df.dropna(subset=['text'], inplace=True)\n",
    "        labeled_df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "        unlabeled_df.to_csv(os.path.join(output_filepath + '_unlabeled', Path(file).stem) + '.csv', index=False)\n",
    "            \n",
    "        unlabeled_df_all = unlabeled_df if unlabeled_df_all is None else pd.concat([unlabeled_df_all, unlabeled_df], ignore_index=True)\n",
    "        \n",
    "        labeled_df.to_csv(os.path.join(output_filepath + '_labeled', Path(file).stem + '_labeled.csv'), index=False)\n",
    "        \n",
    "        labeled_df_all = labeled_df if labeled_df_all is None else pd.concat([labeled_df_all, labeled_df], ignore_index=True)\n",
    "         \n",
    "    unlabeled_df_all.to_csv(os.path.join(output_filepath + '_unlabeled', 'all_unlabeled.csv'), index=False)\n",
    "        \n",
    "    labeled_df_all.to_csv(os.path.join(output_filepath + '_labeled', 'all_labeled.csv'), index=False)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def get_ekphrasis_preprocessor():\n",
    "    return TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    "    )\n",
    "\n",
    "def preprocess_rows(df):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info('started preprocessing rows')\n",
    "\n",
    "    df = df.fillna({\n",
    "                        'Title': '',\n",
    "                        'Body': '',\n",
    "                        'Labels': ''\n",
    "                   })\n",
    "    df['text'] = df['Title'] + ' ' + df['Body']\n",
    "    \n",
    "    unlabeled_df = df[~df[\"Labels\"].str.contains(\"bug|documentation|docs|question|enhancement|feature\", regex=True)]\n",
    "    labeled_df = df[df[\"Labels\"].str.contains(\"bug|documentation|docs|question|enhancement|feature\", regex=True)]\n",
    "    \n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(labeled_df['Labels']):\n",
    "#         appear = False\n",
    "        if \"bug\" in label:\n",
    "            labels.append(\"bug\")\n",
    "#             if appear is False:\n",
    "#                 appear = True\n",
    "#             else: raise Exception(label + str(i))\n",
    "        \n",
    "        elif \"doc\" in label:\n",
    "            labels.append(\"documentation\")\n",
    "                \n",
    "        elif \"question\" in label:\n",
    "            labels.append(\"question\")\n",
    "                \n",
    "        elif \"enhancement\" in label:\n",
    "            labels.append(\"enhancement\")\n",
    "                \n",
    "        elif \"feature\" in label:\n",
    "            labels.append(\"feature\")\n",
    "        \n",
    "    labeled_df['Labels'] = labels    \n",
    "    labeled_df[\"label\"] = labeled_df['Labels'].map(label2int).tolist()\n",
    "    \n",
    "    unlabeled_df = unlabeled_df.drop([\"Labels\"], axis=1)\n",
    "    labeled_df = labeled_df.drop([\"Labels\"], axis=1)\n",
    "    \n",
    "    unlabeled_df = unlabeled_df.filter(['text', 'label'])\n",
    "    labeled_df = labeled_df.filter(['text', 'label'])\n",
    "    text_processor = get_ekphrasis_preprocessor()\n",
    "    unlabeled_df['text'] = [clean_text(text, text_processor) for text in tqdm(unlabeled_df['text'])]\n",
    "    labeled_df['text'] = [clean_text(text, text_processor) for text in tqdm(labeled_df['text'])]\n",
    "    return unlabeled_df, labeled_df\n",
    "\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException\n",
    "\n",
    "def clean_text(text, text_processor):\n",
    "    \n",
    "    #bar.set_description('regex')\n",
    "    cleaned = text\n",
    "    cleaned = re.sub('\\*{2}Checklist.+?\\*{2}.+?\\*{2}.+?\\*{2}', ' ', cleaned, flags=re.DOTALL) # Remove Checklist\n",
    "    cleaned = re.sub(r\"\\*{2}.+\\*{2}\",\"<section>\", cleaned)\n",
    "    cleaned = re.sub(image_regex, r'\\1 <img>', cleaned)\n",
    "    cleaned = re.sub(link_regex_1, r'\\1 <url>', cleaned)\n",
    "    cleaned = re.sub(link_regex_2, r'\\1 <url>', cleaned)\n",
    "    cleaned = re.sub(code_regex, '<code>', cleaned)\n",
    "    \n",
    "    \n",
    "#     cleaned = cleaned.replace('**Describe the contribution**', '')\n",
    "#     cleaned = cleaned.replace('**Checklist**', '')\n",
    "#     cleaned = cleaned.replace('**Testing Performed**', '')\n",
    "#     cleaned = cleaned.replace('**Expected Behavior Changes**', '')\n",
    "#     cleaned = cleaned.replace('**Contributor Info**', '')\n",
    "    \n",
    "#     cleaned = cleaned.replace('**Describe the bug**', '')\n",
    "#     cleaned = cleaned.replace('**Expected Behavior**', '')\n",
    "#     cleaned = cleaned.replace('**Reporter Info**', '')\n",
    "#     cleaned = cleaned.replace('**Checklist (Please check before submitting)**', '')\n",
    "#     cleaned = cleaned.replace('**System(s) tested on**', '')\n",
    "#     cleaned = cleaned.replace('**Additional context**', '')\n",
    "#     cleaned = cleaned.replace('**Contributor Info**', '')\n",
    "    \n",
    "    #bar.set_description('ekph')\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    \n",
    "    signal.alarm(5)\n",
    "    \n",
    "    try:\n",
    "        cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
    "    except (RecursionError, TimeoutException):\n",
    "        cleaned = pd.NA\n",
    "    else:\n",
    "        signal.alarm(0)\n",
    "    #bar.set_description('end')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f2e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('making final data set from raw data')\n",
    "\n",
    "\n",
    "\n",
    "preprocess_raw(directory='./issues', output_filepath='./data')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134009d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 60 with multiple labels in NASA#osal, with ~700 issues labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bef9d9",
   "metadata": {},
   "source": [
    "## Checking Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed93ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_unlabeled/nasa#osal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "714520b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('./issues/nasa#osal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6437530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix #92, Refactor `LC_CreateTaskCDS()` to remove multiple returns\n",
      "**Checklist**\r\n",
      "* [x] I reviewed the [Contributing Guide](https://github.com/nasa/osal/blob/main/CONTRIBUTING.md).\r\n",
      "* [x] I signed and emailed the appropriate [Contributor License Agreement](https://github.com/nasa/cFS/blob/main/CONTRIBUTING.md#contributor-license-agreement-cla) to GSFC-SoftwareRelease@mail.nasa.gov and copied cfs-program@lists.nasa.gov.\r\n",
      "\r\n",
      "**Describe the contribution**\r\n",
      "- Fixes #92 \r\n",
      "  - Simple refactor removes multiple returns from `LC_CreateTaskCDS()`\r\n",
      "\r\n",
      "**Testing performed**\r\n",
      "GitHub CI actions all passing successfully (incl. Build + Run, Unit/Coverage Tests etc.).\r\n",
      "\r\n",
      "**Expected behavior changes**\r\n",
      "No change to logic/behavior\r\n",
      "\r\n",
      "**Contributor Info**\r\n",
      "Avi Weiss @thnkslprpt\n",
      " \n",
      "fix # <number> , refactor <code> to remove multiple returns <section> - fixes # <number> - simple refactor removes multiple returns from <code> <section> github ci actions all passing successfully ( incl . build + run , unit / coverage tests etc . ) . <section> no change to logic / behavior <section> avi weiss <user>\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(data1['Title'].iloc[i] + '\\n' + data1['Body'].iloc[i] + '\\n \\n' + data['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aabf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
